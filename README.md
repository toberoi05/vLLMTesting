# vLLMTesting

I have attached below the Google Colab that contains a sample LLM configuration (using CodeLlama-7b) using vLLM. To run it, simply run the Google Colab one cell at a time, and then when user input is prompted, provide a prompt that you want the LLM to produce a Python script for (ie: Compute the first 10 prime numbers).

I used a A100 GPU on Google Colab to run the LLM, and it used up ~36/40 GPU RAM, so GPU's with less space will not be sufficient.

Google Colab: https://colab.research.google.com/drive/1LaN-eWRBBpgy6av3_g8Q2hUbuiO2ReQJ?usp=sharing
